# LLM Finetuner - A Reusable Pipeline for Fine-Tuning LLMs

The motivation of this project was the desire to be able to easily and continually fine-tune an LLM for a specialized task. I designed and implemented a modular, reusable pipeline for fine-tuning such large language models. I wanted to be able to post-train LLMs without access to much expensive hardware. My solution was to adapt a small general-purpose model into a specialized tool, good at one particular task.

The pipeline consisted of multiple fine-tuning techniques including chain-of-thought prompting, supervised learning, and rejection sampling. Given a fine-tuning dataset, this strategy enables continual improvement of the base model by iteratively refining its reasoning and response quality. Chain-of-thought prompting teaches the model to reason through its answers step-by-step, while rejection sampling ensures that only high-quality responses based on human judgment are used for further training. Because the pipeline supports resampling, we can continually enhance the dataset and model performance by simply modifying the chain-of-thought prompt. This allows the model to develop more sophisticated reasoning over time without requiring new datasets or changes to the model architecture.
