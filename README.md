# LLM Finetuner

This pipeline consisted of multiple fine-tuning techniques including chain-of-thought prompting, supervised learning, and rejection sampling. Given a fine-tuning dataset, this strategy enables continual improvement of the base model by iteratively refining its reasoning and response quality. Chain-of-thought prompting teaches the model to reason through its answers step-by-step, while rejection sampling ensures that only high-quality responses based on human judgment are used for further training. Because the pipeline supports resampling, we can continually enhance the dataset and model performance by simply modifying the chain-of-thought prompt. This allows the model to develop more sophisticated reasoning over time without requiring new datasets or changes to the model architecture.